#cloud-config

hostname: "{{ ipaddress }}"
coreos:
  flannel:
    interface: {{ ipaddress }}
  etcd2:
    discovery: {{ discoveryid }}
    advertise-client-urls: https://{{ ipaddress }}:2379
    initial-advertise-peer-urls: https://{{ ipaddress }}:2380
    listen-client-urls: http://127.0.0.1:2379,https://{{ ipaddress }}:2379
    listen-peer-urls: https://{{ ipaddress }}:2380
    cert-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
    key-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
    trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    client-cert-auth: true
    peer-cert-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
    peer-key-file: /etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
    peer-trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    peer-client-cert-auth: true
{% if isworker == 1 %}
    proxy: on
{% endif %}
  fleet:
    metadata: "role=node"
  units:
    - name: etcd2.service
      command: stop
    - name: fleet.service
      command: stop
    - name: iptables-restore.service
      command: stop
    - name: 00-enp0s3.network
      runtime: true
      content: |
        [Match]
        Name=ens32
        [Network]
        DNS={{ dnsserver }}
        Address={{ ipaddress }}
        Gateway={{ ipaddressgw }}
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
      command: start
    - name: flanneld.service
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            After=etcd2.service
            [Service]
{% if isworker != 1 %}
            ExecStartPre=/usr/bin/etcdctl --cert-file=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem --key-file=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem --ca-file=/etc/kubernetes/ssl/etcd-ca.pem set /coreos.com/network/config '{ "Network": "10.244.0.0/16", "Backend":{"Type":"vxlan"}}'
{% endif %}
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      command: start
    - name: kubelet.service
      content: |
        [Unit]
        Requires=docker.service
        After=docker.service
        [Service]
        Environment=KUBELET_IMAGE_TAG={{ k8sver }}
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
{% if netoverlay == "calico" %}
        --volume cni-net,kind=host,source=/etc/cni/net.d \
        --mount volume=cni-net,target=/etc/cni/net.d \
        --volume cni-bin,kind=host,source=/opt/cni/bin \
        --mount volume=cni-bin,target=/opt/cni/bin \
{% endif %}
        --hosts-entry=host"
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
{% if netoverlay == "calico" %}
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /etc/cni/net.d
{% endif %}
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml \
        --require-kubeconfig \
{% if isworker != 1 %}
        --register-schedulable=false \
{% else %}
        --register-node=true \
{% endif %}
{% if netoverlay == "calico" %}
        --cni-conf-dir=/etc/cni/net.d \
        --cni-bin-dir=/opt/cni/bin \
{% else %}
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
{% endif %}
        --network-plugin=cni \
        --container-runtime=docker \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override={{ ipaddress }} \
        --cluster_dns=10.3.0.10 \
        --cluster_domain=cluster.local \
        --client-ca-file=/etc/kubernetes/ssl/ca.pem \
        --anonymous-auth=false \
        --cloud-provider={{ cloudprovider }} \
        --cloud-config=/etc/kubernetes/cloud.conf
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
      command: start
    - name: increase-nf_conntrack-connections.service
      command: start
      content: |
        [Unit]
        Description=Increase the number of connections in nf_conntrack.
        [Service]
        Type=oneshot
        ExecStartPre=/usr/sbin/modprobe nf_conntrack
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_max=589824"
    - name: increase-nf_conntrack-hashsize.service
      command: start
      content: |
        [Unit]
        Description=Increase the nf_conntrack hashsize.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 147456 > /sys/module/nf_conntrack/parameters/hashsize"
    - name: increase-port_range.service
      command: start
      content: |
        [Unit]
        Description=Increase port_range.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range"
    - name: increase-net.core.somaxconn.service
      command: start
      content: |
        [Unit]
        Description=Increase net.core.somaxconn.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.core.somaxconn=256"
    - name: change-conntrack_timeout.service
      command: start
      content: |
        [Unit]
        Description=change conntrack tcp timeout.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=1"
    - name: change-tcp_timeout_estab.service
      command: start
      content: |
        [Unit]
        Description=change tcp timeout estab.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=600"
    - name: settimezone.service
      command: start
      content: |
        [Unit]
        Description=Set the time zone
        [Service]
        ExecStart=/usr/bin/timedatectl set-timezone Europe/Amsterdam
        RemainAfterExit=yes
        Type=oneshot
    - name: systemd-timesyncd.service
      command: stop
      mask: true
    - name: ntpd.service
      command: start
      enable: true
    - name: systemd-modules-load.service
      command: restart
    - name: systemd-sysctl.service
      command: restart
  update:
    reboot-strategy: "etcd-lock"
  locksmith:
    window-start: Thu 04:00
    window-length: 1h
users:
  - name: "core"
    passwd: "USER_CORE_PASSWORD"
    groups:
      - "sudo"
      - "docker"
write_files:
  - path: "/home/core/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD
  - path: "/root/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD:/home/core
  - path: "/var/lib/iptables/rules-save"
    permissions: "0644"
    owner: "root"
    content: |
      *filter
      :INPUT DROP [0:0]
      :FORWARD DROP [0:0]
      :OUTPUT ACCEPT [0:0]
      -A INPUT -i lo -j ACCEPT
      -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2379 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2380 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8472 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8285 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT
      COMMIT
  - path: "/etc/resolv.conf"
    permissions: "0644"
    owner: "root"
    content: |
      nameserver {{ dnsserver }}
  - path: "/home/core/getkube.sh"
    permissions: "0644"
    owner: "core"
    content: |
     #!/bin/bash
     curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
     chmod +x kubectl
  - path: "/etc/kubernetes/cni/docker_opts_cni.env"
    permissions: "0644"
    owner: "root"
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
  - path: "/etc/kubernetes/cni/net.d/10-flannel.conf"
    permissions: "0644"
    owner: "root"
    content: |
      {
        "name": "podnet",
        "type": "flannel",
        "delegate": {
          "isDefaultGateway": true
        }
      }
  - path: "/etc/flannel/options.env"
    permissions: "0644"
    owner: "root"
    content: |
      FLANNELD_IFACE={{ ipaddress }}
      FLANNELD_ETCD_ENDPOINTS={{ etcdendpointsurls }}
      FLANNELD_ETCD_KEYFILE=/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem
      FLANNELD_ETCD_CERTFILE=/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem
      FLANNELD_ETCD_CAFILE=/etc/ssl/certs/etcd-ca.pem
      FLANNEL_IMAGE_TAG={{ flannelver }}
{% if isworker != 1 %}
  - path: "/etc/kubernetes/manifests/kube-apiserver.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers={{ etcdendpointsurls }}
          - --etcd-cafile=/etc/kubernetes/ssl/etcd-ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem
          - --apiserver-count={{ managers }}
          - --allow-privileged=true
          - --storage-backend=etcd2
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          - --advertise-address={{ ipaddress }}
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction
          - --tls-cert-file=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/sa-{{ clustername }}-k8s.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true
          - --kubelet-client-certificate=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
          - --kubelet-client-key=/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
          - --anonymous-auth=false
          - --authorization-mode={{ authmode }}
          - --v=4
          - --cloud-provider={{ cloudprovider }}
          - --cloud-config=/etc/kubernetes/ssl/cloud.conf
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
{% endif %}
  - path: "/etc/kubernetes/manifests/kube-proxy.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - proxy
{% if isworker == 1 %}
          - --master=https://{{ loadbalancer }}
{% else %}
          - --master=http://127.0.0.1:8080
{% endif %}
          - --cluster-cidr=10.244.0.0/16
          - --conntrack-max-per-core=0
          - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /etc/kubernetes/master-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/master-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
{% if isworker != 1 %}
  - path: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/sa-{{ clustername }}-k8s-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider={{ cloudprovider }}
          - --cloud-config=/etc/kubernetes/ssl/cloud.conf
          - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/master-kubeconfig.yaml
            name: kubeconfig
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: etc-kube-ssl
            readOnly: true
        volumes:
        - name: kubeconfig
          hostPath:
            path: /etc/kubernetes/master-kubeconfig.yaml
        - name: etc-kube-ssl
          hostPath:
            path: /etc/kubernetes/ssl
  - path: "/etc/kubernetes/manifests/kube-scheduler.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:{{ k8sver }}
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --kubeconfig=/etc/kubernetes/master-kubeconfig.yaml
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /etc/kubernetes/master-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/master-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
{% if netoverlay == "calico" %}
  - path: "/etc/kubernetes/manifests/calico.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      # This ConfigMap is used to configure a self-hosted Calico installation.
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: calico-config
        namespace: kube-system
      data:
        # Configure this with the location of your etcd cluster.
        etcd_endpoints: "{{ etcdendpointsurls }}"

        # Configure the Calico backend to use.
        calico_backend: "bird"

        # The CNI network configuration to install on each node.
        cni_network_config: |-
          {
              "name": "k8s-pod-network",
              "cniVersion": "0.1.0",
              "type": "calico",
              "etcd_endpoints": "__ETCD_ENDPOINTS__",
              "etcd_key_file": "__ETCD_KEY_FILE__",
              "etcd_cert_file": "__ETCD_CERT_FILE__",
              "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
              "log_level": "info",
              "mtu": 1450,
              "ipam": {
                  "type": "calico-ipam"
              },
              "policy": {
                  "type": "k8s",
                  "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                  "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
              },
              "kubernetes": {
                  "kubeconfig": "__KUBECONFIG_FILEPATH__"
              }
          }

        # If you're using TLS enabled etcd uncomment the following.
        # You must also populate the Secret below with these files.
        etcd_ca: "/calico-secrets/etcd-ca"   # "/calico-secrets/etcd-ca"
        etcd_cert: "/calico-secrets/etcd-cert" # "/calico-secrets/etcd-cert"
        etcd_key: "/calico-secrets/etcd-key"  # "/calico-secrets/etcd-key"

      ---

      # The following contains k8s Secrets for use with a TLS enabled etcd cluster.
      # For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: calico-etcd-secrets
        namespace: kube-system
      data:
        # Populate the following files with etcd TLS configuration if desired, but leave blank if
        # not using TLS for etcd.
        # This self-hosted install expects three files with the following names.  The values
        # should be base64 encoded strings of the entire contents of each file.
        etcd-key: {{ etcdnodekeybase64 }}
        etcd-cert: {{ etcdnodebase64 }}
        etcd-ca: {{ etcdcabase64 }}

      ---

      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: extensions/v1beta1
      metadata:
        name: calico-node
        namespace: kube-system
        labels:
          k8s-app: calico-node
      spec:
        selector:
          matchLabels:
            k8s-app: calico-node
        template:
          metadata:
            labels:
              k8s-app: calico-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            serviceAccountName: calico-node
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: quay.io/calico/node:v2.6.2
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Choose the backend to use.
                  - name: CALICO_NETWORKING_BACKEND
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: calico_backend
                  # Cluster type to identify the deployment type
                  - name: CLUSTER_TYPE
                    value: "k8s,bgp"
                  # Disable file logging so `kubectl logs` works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  # Set Felix endpoint to host default action to ACCEPT.
                  - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                    value: "ACCEPT"
                  # Configure the IP Pool from which Pod IPs will be chosen.
                  - name: CALICO_IPV4POOL_CIDR
                    value: "10.244.0.0/16"
                  - name: CALICO_IPV4POOL_IPIP
                    value: "always"
                  # Disable IPv6 on Kubernetes.
                  - name: FELIX_IPV6SUPPORT
                    value: "false"
                  # Set Felix logging to "info"
                  - name: FELIX_LOGSEVERITYSCREEN
                    value: "DEBUG"
                  # Set MTU for tunnel device used if ipip is enabled
                  - name: FELIX_IPINIPMTU
                    value: "1430"
                  # Location of the CA certificate for etcd.
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  # Location of the client key for etcd.
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  # Location of the client certificate for etcd.
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
                  # Auto-detect the BGP IP address.
                  - name: IP
                    value: ""
                  - name: FELIX_HEALTHENABLED
                    value: "true"
                securityContext:
                  privileged: true
                resources:
                  requests:
                    cpu: 250m
                livenessProbe:
                  httpGet:
                    path: /liveness
                    port: 9099
                  periodSeconds: 10
                  initialDelaySeconds: 10
                  failureThreshold: 6
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 9099
                  periodSeconds: 10
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: true
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /calico-secrets
                    name: etcd-certs
              # This container installs the Calico CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: quay.io/calico/cni:v1.11.0
                command: ["/install-cni.sh"]
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: cni_network_config
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
                  - mountPath: /calico-secrets
                    name: etcd-certs
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/cni/net.d
              # Mount in the etcd TLS secrets.
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets

      ---

      # This manifest deploys the Calico Kubernetes controllers.
      # See https://github.com/projectcalico/kube-controllers
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: calico-kube-controllers
        namespace: kube-system
        labels:
          k8s-app: calico-kube-controllers
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
          scheduler.alpha.kubernetes.io/tolerations: |
            [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
             {"key":"CriticalAddonsOnly", "operator":"Exists"}]
      spec:
        # The controllers can only have a single active instance.
        replicas: 1
        strategy:
          type: Recreate
        template:
          metadata:
            name: calico-kube-controllers
            namespace: kube-system
            labels:
              k8s-app: calico-kube-controllers
          spec:
            # The controllers must run in the host network namespace so that
            # it isn't governed by policy that would prevent it from working.
            hostNetwork: true
            serviceAccountName: calico-kube-controllers
            containers:
              - name: calico-kube-controllers
                image: quay.io/calico/kube-controllers:v1.0.0
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Location of the CA certificate for etcd.
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  # Location of the client key for etcd.
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  # Location of the client certificate for etcd.
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
                volumeMounts:
                  # Mount in the etcd TLS secrets.
                  - mountPath: /calico-secrets
                    name: etcd-certs
            volumes:
              # Mount in the etcd TLS secrets.
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets

      ---

      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: calico-kube-controllers
        namespace: kube-system

      ---

      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: calico-node
        namespace: kube-system
{% endif %}        
  - path: "/etc/kubernetes/ssl/sa-{{ clustername }}-k8s-key.pem"
    permissions: "0600"
    encoding: "base64"
    owner: "root"
    content: |
      {{ sak8skeybase64 }}
  - path: "/etc/kubernetes/ssl/sa-{{ clustername }}-k8s.pem"
    permissions: "0600"
    encoding: "base64"
    owner: "root"
    content: |
      {{ sak8sbase64 }}
{% endif %}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem"
    permissions: "0600"
    encoding: "base64"
    owner: "root"
    content: |
      {{ k8snodekeybase64 }}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ k8snodebase64 }}
  - path: "/etc/kubernetes/ssl/ca.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ cabase64 }}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node-key.pem"
    permissions: "0644"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodekeybase64 }}
  - path: "/etc/kubernetes/ssl/{{ ipaddress }}-etcd-node.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodebase64 }}
  - path: "/etc/kubernetes/ssl/etcd-ca.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdcabase64 }}
  - path: "/etc/ssl/certs/{{ ipaddress }}-etcd-node-key.pem"
    permissions: "0644"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodekeybase64 }}
  - path: "/etc/ssl/certs/{{ ipaddress }}-etcd-node.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdnodebase64 }}
  - path: "/etc/ssl/certs/etcd-ca.pem"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ etcdcabase64 }}
  - path: "/etc/kubernetes/cloud.conf"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ cloudconfbase64 }}
  - path: "/etc/kubernetes/ssl/cloud.conf"
    permissions: "0664"
    encoding: "base64"
    owner: "root"
    content: |
      {{ cloudconfbase64 }}
  - path: /etc/motd.d/k8s.conf
    owner: "root"
    permissions: "0644"
    encoding: "gzip+base64"
    content: |
      H4sIAJ7IHVkAA3WTO7rEIAiF+6zCng/oWQ07oKDK7u9Bx6iTuTTx8R8CAq19mzOLv05h1/eBsEbed2YYvxQ77WKUIKdBoafi+kkSP8tSyEmfPu/b2rGFwhfN5xWRNKU4BLHo6HuxvJPY3aUiw0ejDqzjPGn++DRBTkAyERt+YRAIUy7n13RdvDTQuC261BUszVhk0DL33CRUqjaIxGoR2nz6okFPdfZlasVWz8zRPdrm/Go+c0dkpFaP61EalECJ16+1aO1+FVzVXZitQCwhrG5xg6gzDjo/dCDIHhUViXSrWyqVoEEjr6utIt/qbhFeR+R4RFRJZpJ4+fLdXB8+qjYgqp/wBzz+djVruRygov5YW2yORhld5TbPzQl1gWUmP15Czo6VkUjyUzts7HNo72ngo+c2Da2B2GZny3Zje3a/Jg0z+TLZ799T/K9df0AUN+scBAAA
  - path: "/etc/kubernetes/master-kubeconfig.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
{% if isworker == 1 %}
          server: https://{{ loadbalancer }}
{% else %}
          server: https://{{ ipaddress }}
{% endif %}
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-node.pem
          client-key: /etc/kubernetes/ssl/{{ ipaddress }}-k8s-node-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
  - path: /etc/ntp.conf
    content: |
      server 0.europe.pool.ntp.org
      server 1.europe.pool.ntp.org
      server 2.europe.pool.ntp.org
      # - Allow only time queries, at a limited rate.
      # - Allow all local queries (IPv4, IPv6)
      restrict default nomodify nopeer noquery limited kod
      restrict 127.0.0.1
      restrict [::1]
